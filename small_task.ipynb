{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "small_task.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e6545cbc58e44e859ff8e9958187c83d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_05747e7cf3bf4f6bb78387f319f038e7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_418f998cab734d4fa951af5e3bf5d3b6",
              "IPY_MODEL_184626a9f1b44c2a976417b53f23ac0d"
            ]
          }
        },
        "05747e7cf3bf4f6bb78387f319f038e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "418f998cab734d4fa951af5e3bf5d3b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_370113ccf40c4cd98d065638b38d256e",
            "_dom_classes": [],
            "description": "Training: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 11,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 11,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3db68d9806c84923ac860c06c5905a11"
          }
        },
        "184626a9f1b44c2a976417b53f23ac0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_de6e7173a987454cb56e97c7b9598343",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 11/11 [00:04&lt;00:00,  2.40it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a61f4d64533840ad92a086d4dedb5ccb"
          }
        },
        "370113ccf40c4cd98d065638b38d256e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3db68d9806c84923ac860c06c5905a11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "de6e7173a987454cb56e97c7b9598343": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a61f4d64533840ad92a086d4dedb5ccb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-HYnOmckxEP"
      },
      "source": [
        "# NLP classification task (human vs machine translation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwC3DyrlmpNr"
      },
      "source": [
        "import io\n",
        "import os\n",
        "\n",
        "with io.open(\"test.txt\", encoding='utf8') as real_file:\n",
        "  test_raw = real_file.read().split(\"\\n\")\n",
        "with io.open(\"train.txt\", encoding='utf8') as real_file:\n",
        "  train_raw = real_file.read().split(\"\\n\")"
      ],
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJhmTAff2dHM"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M20Jj13tm8Da"
      },
      "source": [
        "from nltk import word_tokenize\n",
        "def extract_data(data):\n",
        "    targets = []\n",
        "    refs = []\n",
        "    cands = []\n",
        "    scores = []\n",
        "    labels = []\n",
        "    for i in range(int(len(data)/6)):\n",
        "      shift = i*6\n",
        "      targets.append(data[shift])\n",
        "      refs.append(data[shift+1])\n",
        "      cands.append(data[shift+2])\n",
        "      scores.append([float(data[shift+3])])\n",
        "      labels.append(0 if data[shift+4] == 'M' else 1) #0 machine/ 1 human\n",
        "      \n",
        "    return targets, refs, cands, scores, labels"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9-QRb7ftGup"
      },
      "source": [
        "targets, refs, cands, scores, labels = extract_data(train_raw)\n",
        "targets_test, refs_test, cands_test, scores_test, labels_test = extract_data(test_raw)"
      ],
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLeV3lCTgyYX"
      },
      "source": [
        "def f1_metric(matching, predicted, gold):\n",
        "  precision = matching/predicted\n",
        "  recall = matching/gold\n",
        "  f1 = 2 * precision * recall / (precision + recall)\n",
        "  return f1\n",
        "  \n",
        "def f1_metrics(pred, gold):\n",
        "  matching_1 = 0\n",
        "  matching_0 = 0\n",
        "  n_1 = sum(pred)\n",
        "  n_0 = len(pred) - n_1\n",
        "  \n",
        "  for i in range(len(pred)):\n",
        "    if pred[i]==gold[i]:\n",
        "      if pred[i] == 1:\n",
        "        matching_1 += 1\n",
        "      else:\n",
        "        matching_0 += 1\n",
        "  \n",
        "  h_f1 = f1_metric(matching_1, n_1, len(gold))\n",
        "  m_f1 = f1_metric(matching_0, n_0, len(gold))\n",
        "  return h_f1, m_f1\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQL1JWuP8eZE"
      },
      "source": [
        "# Logistic Regression Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJ6uYczP8c8O"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import coo_matrix, hstack\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNBptHMA8dge"
      },
      "source": [
        "#Extract n-grams\n",
        "vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(1, 4))\n",
        "tfid_features = vectorizer.fit_transform(cands)\n",
        "\n",
        "#Combine features\n",
        "X = hstack([tfid_features,scores]).toarray()\n",
        "Y = labels"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyLtBduQYwaT"
      },
      "source": [
        "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, random_state=42, test_size=0.2)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9zGN3I5C1zw",
        "outputId": "c90e3498-c06a-4dd6-e10b-9b2d8781a082"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "#Initialize model\n",
        "clf = LogisticRegression(penalty = 'l2', solver='liblinear', C=100)\n",
        "\n",
        "#Train and validate\n",
        "clf.fit(X_train, Y_train)\n",
        "acc = clf.score(X_val, Y_val)\n",
        "print(f\"val accuracy: {acc}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "val accuracy: 0.7863247863247863\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnJQktsjZRmZ",
        "outputId": "c5adc9a5-956b-4013-d5c8-b6561015bcde"
      },
      "source": [
        "#TEST\n",
        "tfid_features_test = vectorizer.transform(cands_test)\n",
        "X_test = hstack([tfid_features_test,coo_matrix(scores_test)]).toarray()\n",
        "Y_test = labels_test\n",
        "acc = clf.score(X_test, Y_test)\n",
        "print(f\"train accuracy: {acc}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train accuracy: 0.7528735632183908\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zQw1rznrZ0a",
        "outputId": "c43fed82-6cc0-49de-cd78-8bbf020895b2"
      },
      "source": [
        "#f1 metrics\n",
        "predictions = clf.predict(X_test)\n",
        "h_f1, m_f1 = f1_metrics(predictions, Y_test)\n",
        "print(f\"human f1: {h_f1}\")\n",
        "print(f\"machine f1: {m_f1}\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "human f1: 0.5136186770428016\n",
            "machine f1: 0.4905660377358491\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfFpyAxg-i42"
      },
      "source": [
        "# LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4x2IIKqa-QK"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uq9786Hpbqkf"
      },
      "source": [
        "!unzip glove*.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbnAzZkybrB0"
      },
      "source": [
        "import torch\n",
        "\n",
        "#load glove weights and dictionary\n",
        "dim = 100\n",
        "words = []\n",
        "idx = 0\n",
        "word2idx = {}\n",
        "vectors = []\n",
        "\n",
        "with open(f'glove.6B.{dim}d.txt', 'rb') as f:\n",
        "    for l in f:\n",
        "        line = l.decode().split()\n",
        "        word = line[0]\n",
        "        words.append(word)\n",
        "        word2idx[word] = idx\n",
        "        idx += 1\n",
        "        vect = np.array(line[1:]).astype(np.float)\n",
        "        vectors.append(vect)\n",
        "\n",
        "#Handle unknown words by adding an 'UKN' token.\n",
        "unk_token = 'UKN'\n",
        "unk_idx = idx\n",
        "words.append(unk_token)\n",
        "word2idx[unk_token] = unk_idx\n",
        "vectors.append(np.random.normal(scale=0.6, size=(dim, )))\n",
        "\n",
        "#create the weight matrix and glove dictionary\n",
        "glove = {w: vectors[word2idx[w]] for w in words}\n",
        "weights = torch.tensor(vectors, dtype=torch.float)"
      ],
      "execution_count": 382,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7_KHSYYb5Rm"
      },
      "source": [
        "#helper functions for preprocessing\n",
        "from torch import nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# For padding the input data to same length\n",
        "def pad_collate(batch):\n",
        "  (refs, cands, scores, labels) = zip(*batch)\n",
        "  refs_lens = [len(x) for x in refs]\n",
        "  cands_lens = [len(x) for x in cands]\n",
        "\n",
        "  refs_pad = pad_sequence(refs, batch_first=True, padding_value=0)\n",
        "  cands_pad = pad_sequence(cands, batch_first=True, padding_value=0)\n",
        "  return refs_pad,  refs_lens, cands_pad, cands_lens, torch.LongTensor(scores), torch.LongTensor(labels)\n",
        "\n",
        "def get_index(word):\n",
        "  if word in word2idx:\n",
        "    return word2idx[word]\n",
        "  else:\n",
        "    return word2idx[unk_token]\n",
        "\n",
        "def tokenize(text):\n",
        "  doc = nlp(text.lower())\n",
        "  tokens = [get_index(token.text) for token in doc]\n",
        "  return tokens\n"
      ],
      "execution_count": 383,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dn20pgdVcwfR"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class DatasetRnn(Dataset):\n",
        "    def __init__(self, refs, candidates, scores, labels):\n",
        "      self.refs = refs\n",
        "      self.candidates = candidates\n",
        "      self.scores = torch.LongTensor(scores)\n",
        "      self.labels = torch.LongTensor(labels)\n",
        "    \n",
        "    def __len__(self):\n",
        "      return len(self.labels)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "      cand = tokenize(self.candidates[index])\n",
        "      ref = tokenize(self.refs[index])\n",
        "      return torch.tensor(ref), torch.tensor(cand), self.scores[index], self.labels[index]"
      ],
      "execution_count": 384,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_IdWAuOen9y"
      },
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "#submodule to output sentence encoding using Glove + LSTM\n",
        "class SeqEmbedding(nn.Module):\n",
        "  def __init__(self, embedding, word_vec_dim, h):\n",
        "    super(SeqEmbedding, self).__init__()\n",
        "    self.embedding = embedding\n",
        "    self.word_vec_dim = word_vec_dim\n",
        "    self.rnn = nn.LSTM(word_vec_dim, h, batch_first=True)\n",
        "  \n",
        "  def forward(self, inputs, original_lengths):\n",
        "    # Embed using word embeddings and form a PackedSeq object\n",
        "    embeds = self.embedding(inputs)\n",
        "    packed_inputs = pack_padded_sequence(embeds, original_lengths, batch_first=True, enforce_sorted=False)\n",
        "\t\t\n",
        "    # Passing in the input and hidden state into the model and obtaining outputs\n",
        "    out_packed, _ = self.rnn(packed_inputs)\n",
        "    out_padded, out_lengths = pad_packed_sequence(out_packed, batch_first=True)\n",
        "    \n",
        "    # Get the hidden layer for the final word\n",
        "    last_h = []\n",
        "    for i in range(len(out_lengths)):\n",
        "       last_h.append(out_padded[i][out_lengths[i]-1].unsqueeze(0))\n",
        "    last_h = torch.cat(last_h)\n",
        "\n",
        "    return last_h\n",
        "\n",
        "class Model(nn.Module):\n",
        "  def __init__(self, weights, h=64): \n",
        "    super(Model, self).__init__()\n",
        "    vocab_size, word_vec_dim = weights.shape\n",
        "    self.embedding = nn.Embedding.from_pretrained(weights, freeze=True)\n",
        "    self.candEmbedding = SeqEmbedding(self.embedding, word_vec_dim, h)\n",
        "    self.refEmbedding = SeqEmbedding(self.embedding, word_vec_dim, h)\n",
        "    self.ffnn1 = nn.Linear(h * 2 + 1, 2) #feature vector = [ref_emb, cand_emb, score]\n",
        "    self.loss = nn.CrossEntropyLoss()\n",
        "  \n",
        "  def compute_loss(self, predicted_vector, gold_label):\n",
        "    return self.loss(predicted_vector, gold_label)\t\n",
        "  \n",
        "  def forward(self, refs, refs_lengths, cands, cand_lengths, scores):\n",
        "    cand_emb = self.candEmbedding(cands, cand_lengths)\n",
        "    refs_emb = self.refEmbedding(refs, refs_lengths)\n",
        "    x1 = torch.cat((cand_emb, refs_emb, scores.unsqueeze(dim=1)), dim=1)\n",
        "    x2 = self.ffnn1(x1)\n",
        "    return x2"
      ],
      "execution_count": 402,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QynQfbilirUd"
      },
      "source": [
        "train_dataset = DatasetRnn(refs, cands, scores, labels)\n",
        "train_set, val_set = torch.utils.data.random_split(train_dataset, [460, 124])\n",
        "train_loader = DataLoader(train_set, batch_size=16, shuffle=True, collate_fn=pad_collate)\n",
        "val_loader = DataLoader(val_set, batch_size=16, collate_fn=pad_collate)\n",
        "model = Model(weights, h = 64)"
      ],
      "execution_count": 403,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XDdE6j9oVGa"
      },
      "source": [
        "import torch.optim as optim\n",
        "from tqdm.notebook import tqdm, trange\n",
        "import torch, gc\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "def train_epoch(model, train_loader, optimizer):\n",
        "  model.to(device)\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  for (refs, refs_lengths, cands, cand_lengths, scores, y) in tqdm(train_loader, leave=False, desc=\"Training\"):\n",
        "    output = model(refs.to(device), refs_lengths, cands.to(device), cand_lengths, scores.to(device))\n",
        "    loss = model.compute_loss(output, y.to(device))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total_loss = loss.item()\n",
        "  print(f'train loss: {total_loss}')\n",
        "\n",
        "def validate(model, val_loader):\n",
        "\tmodel.eval()\n",
        "\tcorrect = 0\n",
        "\ttotal = 0\n",
        "\tfor (refs, refs_lengths, cands, cand_lengths, scores, y) in tqdm(val_loader, leave=False, desc=\"Validation Batches\"):\n",
        "\t\toutput = model(refs.to(device), refs_lengths, cands.to(device), cand_lengths, scores.to(device))\n",
        "\t\ttotal += output.size()[0]\n",
        "\t\t_, predicted = torch.max(output, 1)\n",
        "\t\tcorrect += (y.to(\"cpu\") == predicted.to(\"cpu\")).cpu().numpy().sum()\n",
        "\tacc = correct/total\n",
        "\tprint(f\"validation accuracy: {acc}\")\n",
        "\n",
        "def train(number_of_epochs, model, train_loader, val_loader):\n",
        "  optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
        "  for epoch in trange(number_of_epochs, desc=\"Epochs\"):\n",
        "    train_epoch(model, train_loader, optimizer)\n",
        "    validate(model, val_loader)"
      ],
      "execution_count": 404,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxL5MkZYojCr"
      },
      "source": [
        "train(5, model, train_loader, val_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yx_8gIbpV0w"
      },
      "source": [
        "test_dataset = DatasetRnn(refs_test, cands_test, scores_test, labels_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, collate_fn=pad_collate)"
      ],
      "execution_count": 391,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtVEBSMThyvX"
      },
      "source": [
        "def predict(model, test_loader):\n",
        "  model.to(device)\n",
        "  predictions = []\n",
        "  truth = []\n",
        "  correct = 0\n",
        "  for (refs, refs_lengths, cands, cand_lengths, scores, y) in tqdm(test_loader, leave=False, desc=\"Training\"):\n",
        "    output = model(refs.to(device), refs_lengths, cands.to(device), cand_lengths, scores.to(device))\n",
        "    _, predicted = torch.max(output, 1)\n",
        "    predictions += predicted.tolist()\n",
        "    truth += y.tolist()\n",
        "    correct += (y.to(\"cpu\") == predicted.to(\"cpu\")).cpu().numpy().sum()\n",
        "  acc = correct/len(truth)\n",
        "  print(f\"test accuracy: {acc}\")\n",
        "  return predictions, truth"
      ],
      "execution_count": 399,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34,
          "referenced_widgets": [
            "e6545cbc58e44e859ff8e9958187c83d",
            "05747e7cf3bf4f6bb78387f319f038e7",
            "418f998cab734d4fa951af5e3bf5d3b6",
            "184626a9f1b44c2a976417b53f23ac0d",
            "370113ccf40c4cd98d065638b38d256e",
            "3db68d9806c84923ac860c06c5905a11",
            "de6e7173a987454cb56e97c7b9598343",
            "a61f4d64533840ad92a086d4dedb5ccb"
          ]
        },
        "id": "ARlfrHHRi1Ml",
        "outputId": "87578d18-51a5-4533-daf4-f1e5356c4a27"
      },
      "source": [
        "predictions, truth = predict(model, test_loader)"
      ],
      "execution_count": 400,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e6545cbc58e44e859ff8e9958187c83d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Training', max=11.0, style=ProgressStyle(description_widtâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy: 0.6436781609195402\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kn0F2UAoi7Gb",
        "outputId": "c487e86e-aeaf-415a-e035-ef4d0685aa36"
      },
      "source": [
        "h_f1, m_f1 = f1_metrics(predictions, truth)\n",
        "print(f\"human f1: {h_f1}\")\n",
        "print(f\"machine f1: {m_f1}\")"
      ],
      "execution_count": 401,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "human f1: 0.4285714285714286\n",
            "machine f1: 0.4296296296296296\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}